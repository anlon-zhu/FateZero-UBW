# CUDA_VISIBLE_DEVICES=0 python test_fatezero.py --config config/teaser/jeep_posche.yaml

pretrained_model_path: "./ckpt/highway_500"

dataset_config:
  path: "data/highway"
  prompt: "a white van drives by a highway dashcam video with forests on either side," # RENAME Inversion editing_prompts
  n_sample_frame: 8
  sampling_rate: 1
  stride: 80
  offset:
    left: 0
    right: 0
    top: 0
    bottom: 0

editing_config:
  use_invertion_latents: true
  use_inversion_attention: true

  editing_prompts:
    [
      a white van drives by a dashcam on an autumn forrested highway,
      a Scooby Doo Bus drive by a dashcam on a forrested highway,
      a white van drives by a dashcam on a snowy highway,
      a blue FordGT drives by a dashcam on a desert highway,
      a BMW aggressively cuts off a car on a dashcam on a forrested highway,
    ]
  p2p_config:
    0:
      # Whether to directly copy the cross attention from source
      # True: directly copy, better for object replacement
      # False: keep source attention, better for style
      is_replace_controller: False

      # Semantic layout preserving. High steps, replace more cross attention to preserve semantic layout
      cross_replace_steps:
        default_: 0.8

      # Source background structure preserving, in [0, 1].
      # e.g., =0.6 Replace the first 60% steps self-attention
      self_replace_steps: 0.9

      # Amplify the target-words cross attention, larger value, more close to target
      # Usefull in style editing
      # eq_params: # equilizer_params
      #   words: ["watercolor", "painting"]
      #   values: [10, 10] # RENAME ca_amp

      # Target structure-divergence hyperparames
      # If you change the shape of object better to use all three line, otherwise, no need.
      # Without following three lines, all self-attention will be replaced
      # Usefull in shape editing
      blend_words: [["van"], ["car"]]
      blend_self_attention: True
      # blend_latents: False   #  Directly copy the latents, performance not so good in our case

      # preserve source structure of blend_words , [0, 1]
      # blend_th-> [1.0, 1.0], mask -> 0, use inversion-time attention, the structure is similar to the input
      # blend_th-> [0.0, 0.0], mask -> 1, use more edit self-attention, more generated shape, less source acttention
      blend_th: [0.3, 0.3]

    1:
      cross_replace_steps:
        default_: 0.5
      self_replace_steps: 0.5

      use_inversion_attention: True
      is_replace_controller: True

      # blend_words: [["silver", "jeep"], ["Porsche", "car"]] # for local edit. If it is not local yet - use only the source object: blend_word = ((('cat',), ("cat",))).
      blend_self_attention: True
      blend_th: [0.3, 0.3]

  clip_length: "${..dataset_config.n_sample_frame}"
  sample_seeds: [0]
  prompt2prompt_edit: True # Need study

  # DDIM parameters
  guidance_scale: 7.5
  num_inference_steps: 50

model_config:
  lora: 160

test_pipeline_config:
  target: video_diffusion.pipelines.p2p_ddim_spatial_temporal.P2pDDIMSpatioTemporalPipeline
  num_inference_steps: "${..validation_sample_logger.num_inference_steps}"

seed: 0
